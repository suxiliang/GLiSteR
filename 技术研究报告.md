# 技术研究报告

文档修订历史

| 序号  | 修订原因 | 版本号 | 作者  | 修订日期  | 备注  |
| :---: | :------: | :----: | :---: | :-------: | :---: |
|   1   | 添加内容 | 1.0.0  |  丁   | 2023/1/xx |  22   |

## 问题聚焦

> 初赛阶段完成

### 问题描述

随着生活水平的提高和生活方式的变迁，音乐已经成为许多人生活中重要的一部分。随着互联网与电子设备的发展，人们听音乐的方式由留声机、磁带机到MP3，最终形成现在以智能手机为主的阶段。音乐作为一种雅俗共赏的艺术形式，已经深刻融入人们生活的点滴中，见证听众一天天一年年的生活，成为一段段过往记忆、生活日常的容器，承载用户一条条生活片断中的心情。

> 完犊子不会写了，先暂停这部分。

但是，多数听歌应用仍按照互联网时代前的方式以提供音乐播放能力作为核心功能，围绕音乐播放拓展社交等功能，这其实忽视了音乐在移动互联网时代与个人生活强烈的参与程度。事实上，由于我们可以随时随地戴上耳机，掏出手机开始欣赏音乐，音乐十分深入的参与到我们的生活中，以至于用户欣赏的歌曲、歌单与用户自己此时此刻的心情、情绪有了难舍难分的联系。

一方面，音乐软件的推荐机制普遍根据风格——歌手——流派——听歌次数进行用户模型的建立。然而，用户每天打开听歌应用时的心情是多样的，对一首歌曲的体验是多样的；大量用户在听歌时积累了大量心情和体验，使得一首歌曲一定程度上成为用户共同记忆的载体，使得音乐有了在流派、歌手以外的一种特征。对于用户自身来讲，拥有情绪特点的歌曲以及自身此刻情绪的结合，可以获得更贴心、更懂自己的听歌体验，在某些场景下，用户只了解自己的心情，但不知道自己想听什么。又或者，当用户想要感受图书馆内、候车厅内的氛围时，甚至只是擦肩而过的人都会有共同的心情……在这些场景下用户希望能够有一个能够根据当前心情推荐歌曲的应用。

另一方面，用户对歌曲的欣赏和体验在当下往往是一种单纯的输入过程，事实上，用户需要一些渠道输出自己的想法。主流应用采取的大多是社区设计，但是社区设计的核心是社交，而用户的想法往往是单纯的，用户希望能够有一个能够捕获用户的心情的应用，而不是一个社交平台。

从上述两个方面来看，我们需要做的改进对设备有一定依赖，一些功能需要穿戴设备提供信息，另一些需要PC来提供计算能力。因此，我们需要应用具有跨平台的能力，并且能够自适应地实现信息获取，以及动态的计算方式，从而实现用户的无感体验。

### 问题抽象

> 将项目要解决的具体问题抽象转化为技术问题

解决上述问题的关键在于解决：一是要通过一定的技术手段，将用户的心情和歌曲的情绪特点进行提取分析并结合起来，从而实现用户心情与歌曲情绪特点的匹配；二是要在用户有意愿的情况下，通过更加隐私、无缝的方式实现一定范围内的用户心情和歌曲情绪特点的共享；三是要让用户能够记录自己的心情特征，并通过一定的方式输出自己的想法；四是将信息提取、推荐和创作的功能与设备深入结合，根据场景智能选择计算方式，从而得到体验更流畅的音乐体验。

- 用户与歌曲情绪特点的提取和识别。
- 基于情绪特征、具有场景交互交互能力的歌曲推荐。
- 以用户和歌曲情绪特点为基础的创作系统。
- 智能无感的跨平台信息共享和计算能力。

### 问题定位

> 指出该问题所属的业务领域与技术领域

该问题是一个跨模态特征提取、推荐系统、智能创作和分布式计算相关的综合问题，属于对音乐的情感分析、推荐和创作的一种深入挖掘。本项目旨在挖掘用户与歌曲的情感特征，围绕这一宗旨实现对用户情绪特征的采集、管理统计，歌曲情绪特征的分析、提取，用户与歌曲情绪特征的匹配，以及基于情绪特征的推荐、创作等功能。

从技术上来讲，本项目涉及到的技术领域有：音乐情感分析、文本情感分析、跨模态特征融合、对话任务、推荐系统、智能创作、分布式计算、跨平台信息共享等。

### 问题评估

> 分析问题本身的技术性、普适性、热度等特点。

#### 技术性

本项目的核心需求要做到从文本、音乐等不同模态中提取情绪特征，将多模态下的特征进行特征空间对齐和融合，最终借助智能算法实现情绪特征的生成，并能够在不同场景下动态调整；其次，本项目要实现具有场景感知、环境感知、情绪感知能力的音乐互动，还要求应用具有一定近场通信和交互能力的能力；要实现分层次、具有多种情景下的工作能力的智能推荐，需要设计一个比较灵活强大的推荐系统；要实现智能创作，需要设计一个能够根据用户的情绪特征和歌曲的情绪特征进行创作的系统，并且支持以插件的形式拓展下游任务；要实现智能无感的跨平台信息共享和计算能力，需要提供用户不同设备和云端之间的智能算力调度和信息共享机制，并且在不同平台上提供不同压缩程度的模型部署。

上述过程对应用的开发提出了许多要求，例如客户端在不同平台上的兼容适配、相互调用，以及对不同平台的算力调度和信息共享；服务端的架构配置、功能实现以及对模型的压缩和部署，计算资源的调度管理；客户端的热更新能力，服务端的横向扩展能力、抗压能力等。

本项目在人工智能的应用上，涉及到了多个领域的技术，包括音乐情感分析、文本情感分析、跨模态特征融合、对话任务、推荐系统、智能创作等领域；开发技术上涉及到了多个平台的兼容适配、算力调度和信息共享、模型压缩和部署等技术。这些技术都是比较新的技术，需要对这些技术进行深入的研究和探索，因此本项目具有很强的技术性和挑战性，包含对现有技术的深入研究和对新技术的探索。

#### 普适性

本项目的应用场景是在不同的场景下，根据用户的情绪特征和歌曲的情绪特征进行匹配，从而实现歌曲推荐、智能创作等功能。这些功能都是普适的，不仅可以应用于音乐领域，还可以应用于其他领域，例如电影、电视剧、小说等领域。

本项目涉及的技术也是普适的，能够为多模态特征下的情感分析、跨模态特征融合、推荐系统、智能创作等领域提供技术参考，同时涉及到非标准化信息的保存和处理，不同设备之间的信息共享、算力调度和身份识别等技术，以及服务端的架构配置、功能实现以及对模型的压缩和部署，计算资源的调度管理等，都是许多领域将AI投入生产时需要解决的问题。

因此，本项目的技术方案具有很强的普适性，能够同时为人工智能研究领域和软件开发提供参考。

#### 热度

音乐相关的应用在市场中始终占据着极其重要的地位，从艺术的角度来看，音乐具有十分丰富的信息和强大的扩散、传播能力；近年来伴随人工智能领域的飞速发展，多媒体、多模态等细分领域涌现许多突破性的成果，借助AI技术使得落地一种更”懂“用户的产品具有了可能性；与此同时，在万物互联的大背景下，智能设备之间的交互、信息共享成为新的热点，探索如何利于不同设备的特性构建无感知、高性能、高可用的服务体系同样是我们的目标之一。

### 问题分解

> 根据问题的规模进行分解，将问题分解为若干个子问题，并给出子问题的难度及子问题之间的依赖关系。

#### 用户情绪特征提取

本项目的主要数据支撑就是基于用户的“情绪信息”，其来源非常丰富，包括但不限于：

- 智能穿戴设备检测身体指标
- 智能设备使用情况
- 用户听歌风格、偏好
- 用户与应用的交互

从技术上，需要做到充分利用各种方式采集的数据绘制合适的用户画像，并与音乐的特征空间做好匹配。

#### 多层次、可变且可感知环境的推荐系统

本项目根据使用场景的规模，需要具有以下能力：

- 极近距离下的应用通信和数据分享
- 相同空间内的信息融合和推荐

#### 多设备的信息共享、算力调度

为了实现高校的端云结合与设备互联，充分发挥不同设备的特性，本项目需要实现：

- 复杂计算任务向PC调度
- 不同设备下的模型压缩和部署
- 本地运算和云端服务的协同

#### 优雅的人工智能创作

#### 用户状态感知和引导

需要对检测到用户的心情特征进行相应的反馈。

## 相关工作

> 初赛阶段完成
>
> 罗列至少三项与之相关的已有技术方案，尽可能是近三年内的技术方案。

### 多模态情绪识别

借助多模态特征进行情绪识别是人工智能技术的重要研究方向之一，涉及多模态特征融合、情绪识别等子问题，有非常大的研究潜力，并且具有很高的落地价值。本项目中任务的目标是借助多模态的信息判断用户的情绪。

情感是大脑的高级活动，它是一种复杂的心理和生理状态，高级活动包括记忆、学习、决策和情绪等。情绪是情感的一个外部表现，是我们对事件内在或外在的反应。一个成功的人通常要同时具备高智商和高情商。情商反映一个人控制调节自己情感的能力，以及处理自己与他人之间情感关系的能力。情感很重要，它会影响我们做决策。情感计算要赋予计算机像人一样的观察理解和生成情感特征的能力，最终使得计算机像人一样进行自然亲近和生动的交互。情感计算中基本问题包括情绪识别。

情绪至今没有明确的定义，但是可以简单的理解成“brief brain and body episode that facilitates a response to a significant event”。在计算机领域，要特别注意sentiment analysis（情感分析）的区别，只有当情感的持有者面对一个实体或物体被卷入或唤起的情况时，情感才会表现出来。简单来说就是sentiment必须要有一个客体，而emotion更多的是主观的感受。同时sentiment是一个比较持续的过程，而emotion就更考虑变化。事实上，在面部表情识别中，就有论文利用光流信息提升识别的准确率。

![emotion和feeling](https://img-blog.csdnimg.cn/20210201135908412.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5MzM3MzMy,size_16,color_FFFFFF,t_70)

模态（Modal）是指人接受信息的特定方式，多模态（Multi-Modal）就是不同的信息模态例如语音（包括语义和非语义信息等）、视频（包括RGB信息、深度信息、光流信息等）、文本信息进行自动化系统的学习。与多模态学习联系紧密的跨模态（cross-modal）学习是利用其他模态的信息增强某一个模态信息的学习效果，比如利用视频信息增强对音频数据的训练效果，但是测试时只利用音频数据测试。

在情绪识别的问题中，音频的语义（linguistic）信息是相对不可靠的。一是在指定情绪下，人的词语选择是不确定的；二是语义信息是和语言的种类相关的，情绪却是与语言种类无关的，用语义信息得到的模型是很难泛化的；三是考虑到最后的应用场景（问答式），词语的种类很有限。因此我觉得最后的多模态信息应该排除语音信息，只考虑非语义信息。

多模态学习的融合算法一般可以分为数据融合（data fusion）、特征融合（feature fusion）和决策融合（decision fusion）三个层面。在大数据时代，多模态的数据往往大容量、大速率，且数据形式多样，有结构化、半结构化等等数据，这些数据位于不同的空间中。直接进行融合不仅存在技术上的困难，而且会带来维度灾难和模型收敛的问题。而对于决策融合，一方面由于人们在表述情感信息的时候是冗余的，决策融合往往会带来分类的错误；另一方面，音频信息和图像信息的独立性假设损失了模态间的互信息。

#### 多模态特征融合方法

特征融合有许多可用的方法，包括经典一些的受限/深度玻尔兹曼机、深度神经网络、TransFormer结构的神经网络、图神经网络、门控神经网络等，一下对一些常用的方法进行介绍和讨论。

**受限/深度玻尔兹曼机（Restricted/Deep Boltzmann Machine）**：玻尔兹曼机是一大类的神经网络模型，但是在实际应用中使用最多的则是RBM。RBM本身模型很简单，只是一个两层的神经网络，因此严格意义上不能算深度学习的范畴。不过深度玻尔兹曼机（Deep Boltzmann Machine，以下简称DBM）可以看做是RBM的推广。因为是概率模型，因此此方法可以很好的解决某个模态的输入有缺失的情况。

**卷积/分类器思想的融合**：将多种模态的特征Resize后以一定方式进行拼接，然后使用1x1卷积或全连接层进行特征融合即可得到复合特征。这种方法易于实现并且具有较高的效率，模态特征之间的耦合度也比较低，但是无法适应部分特征缺失的情况，并且难以在深层次上实现特征空间的匹配。

**基于门控单元的神经网络**：参考LSTM、GNU单元中的遗忘门和重置门的设置，设计出门单元来融合多模态信息。

**具有注意力机制/Transformer的融合模块**：注意力机制可以使模型具有很强的自适应能力，做到动态学习和融合特征。利用自注意力机制融合不同模态之间的信息，然后利用门控模块融合不同层级之间的特征。本质上来讲，Transformer也是一种注意力机制的应用，但是具有更好的泛化能力，自提出并应用在NLP问题上设计出BERT模型后，在许多研究领域大放异彩。

#### 多任务融合学习网络(Multitask Fusion Learning Network， MFLN)

吴良庆团队提出一种多任务融合学习网络(Multitask Fusion Learning Network， MFLN)方法来识别多模态情绪。具体而言，将文本信息、图像信息以及声音信息看成是多任务的输人：1)每种模态信息分别通过一个私有的双向LSTM层进行编码，以学习单个模态内部的变化信息；2)3种模态信息之间两两结合形成了种组合，通过共享的双向LSTM层，以学习双模态之间的动态交互作用信息；3)联合3种模态的信息，经过同一个共享双向 LSTM 层来学习3种模态之间的动态联系。最后，把整个网络中学习到的多个模态的内部信息和模态之间的交互信息进行融合，以获取最终的情绪信息。

不同于该任务中的其他的方法，本文从多任务学习的角度探究了多模态情绪识别问题，将模型设计为以下结构:

1. 特征输人层：将多模态内容中的文本、图像和语音特征输人到神经网络。
2. Intra-modality 层：注重于单个模态 (Uni-modal) 内部的信息，采用的是私有的双向LSTM 层。
3. Inter-modality 层：包含两个部分，分别是双模态(Bimodal) 和三模态(Tri-modal)使用的共享双向 LSTM层。
4. 预测分类层：分别使用单模态信息和多模态融合信息对数据集中的所有情绪类别进行识别，并把对每个情绪类别的识别都作为一个二分类任务，即对多个二分类任务进行预测。

![dadsda](https://s2.loli.net/2023/02/22/kKeQ1VcRSOoiUIG.png)

**特征输入层**中，多个模态的时间序列必须对齐后才可以获得最佳性能，如果每次的输人都需要做特征处理，那么这个工作量和时间的消耗都是巨大的。为了得到高效且可靠的数据加载，MFLN采用了卡内基梅隆大学提供的多模态数据 SDK 去获取文本、图像以及语音的特征。文本的特征为 Glove 词向量。图像特征是通过把视频按每秒 30 帧的频率切割成图片，再由 FACET 面部表情分析框架得到。语音特征则是通过把音频按每秒100 帧的频率切分，再由 COVAREP 语音分析框架进行抽取0s。3种模态信息都含有对应的时间戳，在 SDK 中使用 P2FA 方法将文本与音频以及视频对齐119， 即多帧的图片信息和语音信息最后会对应到相应的词的时间间隔。

**Intra- modality 层**主要关注的是单模态内部的信息，本文采用私有的双向LSTM1201 去获取单模态中上下文的语义信息。

**Inter-modality 层**关注的是模态之间的交互作用，分为双模态和三模态之同的交互作用。本文采用共享的双向 LSTM 层去获取模态之间的交互信息。

**预测分类层**分别使用单模态信息和多模态融合信息对多个情绪识别任务进行预测。为了得到文本、图像和语音信息的完整表示，我们首先将 Intra-modality 层和Inter-modality 层学习到的所有文本、图像和语音表示分别融合在一起。

#### Multimodal Information Bottleneck: Learning Minimal Sufficient Unimodal and Multimodal Representations

学习跨模态数据的有效联合嵌入一直是多模态机器学习领域的研究热点。在多模态融合过程中，生成的多模态嵌入可能是冗余的，可判别的单模态信息可能被忽略，这往往会干扰准确的预测，导致过拟合的风险更高。此外，单模态表示还包含对跨模态动力学学习产生负面影响的噪声信息。多模态信息瓶颈(Multi-modal Information Bottleneck, MIB)旨在学习一种强大的、充分的、无冗余的多模态表示，并过滤掉单模态表示中的噪声信息。

具体来说，MIB继承了通用信息瓶颈(Information Bottleneck, IB)，通过最大化表示和目标之间的互信息，同时约束表示和输入数据之间的互信息，来学习给定任务的最小充分表示。与一般的IB不同，MIB规范化了多模态和单模态表示，这是一个全面而灵活的框架，与任何融合方法兼容。C-MIB的网络结构图如下图所示：

![](https://s2.loli.net/2023/02/22/XQaHb1yJC5TFxME.png)

C-MIB采用分层处理的方案，对不同模态信息使用不同的Encoder进行信息提取后，通过Fusion模块得到混合后的特征信息交给后续处理。以前的工作倾向于设计复杂的融合策略来学习具有单模态特征的多模态嵌入，例如张量融合和基于Transformer的融合。这些方法虽然有效，但也有高生成的高维多模态嵌入不可避免地存在冗余，导致过拟合风险较高。理想情况下，多模态嵌入应该1)包含正确预测所需的最大信息，2)包含与预测无关的最少信息。

此外，单模态表示不可避免地包含噪声信息，这可能会对正确的预测产生负面影响。在多模态情感分析和情感识别领域，已有研究表明，语言模态始终处于主导地位，而视觉和听觉模态更像是辅助模态。这些非词汇模态通常包含干扰跨模态交互学习并影响多模态表示质量的噪声。

在信息瓶颈（IB）启发下设计的多模态信息瓶颈(MIB)被训练来学习单模态和多模态表示，旨在获取多模态的丰富特征同时排除不必要的噪声。信息瓶颈(IB)基于相互信息(MD)，目标是最大化编码表示与标签之间的MI，同时最小化编码表示与输入之间的MI。IB能够找到输入的简洁和压缩表示，从信息论的角度考虑信号的复杂性。通过应用IB原理，该模型可以学习滤除可能干扰预测的噪声和冗余信息，从而获得最小的足够表示来进行预测。在我们的工作中，IB用于从跨模态嵌入中提取简洁的表示，丢弃与目标无关的信息。多模态样本携带大量信息，复杂融合方法生成的多模态嵌入存在较大的误差复杂性和高维数，容易导致冗余。

在具体的处理过程中，为了减少生成的多模态表示中的冗余信息，论文里根据融合位置提出了三种融合方式：E-MIB，L-MIB和C-MIB。

早期融合MIB (E-MIB)是指首先通过融合生成主多模态表示，然后在此基础上通过IB原理学习最小充分多模态表示(这里的早期融合不同于通常考虑的特征拼接)。具体来讲，首先提取并融合三种模态的高级单模态表示，得到主关节多模态表示，其中融合方法是灵活的。然后，将主关节多模态表示输入到信息瓶颈，在此基础上对多模态表示进行提取和压缩。理想情况下，压缩的联合表示只包含与任务相关的信息，噪声或冗余信息被过滤掉。因此，压缩的多模态表示没有冗余，过拟合的风险可以降低。

后期融合MIB (L-MIB)被设计用于在单模态表示中过滤掉与预测无关的噪声。这里的后期融合表示我们首先利用IB原理学习最小充分单模态表示，并在此基础上通过融合生成多模态表示(这里的后期融合不同于通常考虑的后期融合，即基于单模态预测投票)。通过这种方法，提取的简练单模态表示法不存在噪声，生成的多模态表示法具有较强的辨识能力。此外，通过应用IB原理，可以减少模态之间的分布差距，从而更好地融合它们。

为了将IB原理全面地注入到多模态系统中，将E-MIB和L-MIB结合起来构建了完整的MIB框架(C-MIB)。C-MIB考虑了过滤单模态表示中的噪声，以及学习一个最小充分的多模态表示，这将是更全面的。事实上，MIB框架是高度灵活的，它与任何融合机制兼容，以提供更高的表达能力。融合机制的比较也在第IV-F节中显示。最后，多模态信息瓶颈(IB)的新架构被设计用来解决多模态学习的问题，具体包含三点:

1. 我们提出了一个新的架构来模拟多模态信号。我们创新地将IB原理注入到多模态学习中，以过滤掉噪声信息，减少冗余。据我们所知，我们是第一个利用IB原理在多模态情感分析和情感识别中学习最小充分单模态和多模态表示的人。
2. E-MIB和L-MIB分别用于学习最小充分多模态表示和过滤单模态表示中的噪声。结合E-MIB和L-MIB的优点，C-MIB可以综合学习更有利的单模态和多模态表示。该MIB框架可与常用的融合策略相结合，具有较高的表达能力。
3. MIB变体在多个数据集上优于其他方法。此外，可视化实验表明，我们的MIB使多模态表示更具鉴别性。此外，我们的方法可以达到最先进的性能，即使是非常简单的融合方法。

### AIGC

#### 图像生成

我们希望利用用户的情绪特征生成一系列艺术作品，其中图像生成就是很重要的一个场景。生成的图像可以用于设备的壁纸、智能穿戴设备的显示以及指导IoT设备渲染一定的居所氛围。

描述一张图像对人类来说相当容易，我们在很小的时候就能做到。在机器学习中，这项任务是一个判别分类/回归问题，即从输入图像预测特征标签，目前已经得到了很好的解决；然而，另一方面，基于描述生成逼真图像却要困难得多，需要多年的平面设计训练。在机器学习中，这是一项生成任务。其最大的特点在于原始数据所含的信息量远远小于所要生成的结果，因此在很长一段时间内没有取得有效的成果。

在2018年，Han Zhang, Ian Goodfellow, Dimitris Metaxas, Augustus Odena提出了自注意力生成对抗网络（SAGAN），它允许注意力驱动的远程依赖建模用于图像生成任务。传统的卷积GAN生成高分辨率细节，仅作为低分辨率特征图中空间局部点的函数。在SAGAN中，可以使用来自所有要素位置的提示生成详细信息。DeepMind 带来的 BigGAN 创造性的将正交正则化的思想引入 GAN，通过对输入先验分布 z 的适时截断大大提升了 GAN 的生成性能，BigGAN 在 SAGAN 的基础上一举将 IS 提高了 100 分，达到了 166 分。

近期，来自慕尼黑大学和 Runway 的研究者基于其 CVPR 2022 的论文《High-Resolution Image Synthesis with Latent Diffusion Models》，并与 Eleuther AI、LAION 等团队合作，共同开发了一种可在消费级 GPU 上运行的文本转图像模型 Stable Diffusion 在世界范围内带来极大震撼。Stable Diffusion 模型是首个在 4000 个 A100 Ezra-1 AI 超大集群上进行训练的文本转图像模型，它可以在消费级 GPU 上的 10 GB VRAM 下运行，并在几秒钟内生成 512x512 像素的图像，无需预处理和后处理，这是速度和质量上的突破。

Stable Diffusion 团队的研究试图利用扩散模型实现文字转图像。尽管扩散模型允许通过对相应的损失项进行欠采样（undersampling）来忽略感知上不相关的细节，但它们仍然需要在像素空间中进行昂贵的函数评估，这会导致对计算时间和能源资源的巨大需求。该研究通过将压缩与生成学习阶段显式分离来规避这个问题，最终降低了训练扩散模型对高分辨率图像合成的计算需求。

#### 音乐生成

音乐创作同样是媒体市场中的重要一环，伴随音乐产业的迅速发展，部分非音乐行业从业者对于快速生成个性定制音乐的需求也在上升。智能音乐创作利用机器学习和神经网络，能够对音频信号进行有效的处理，把握其中的关键特征，从而生成出一大批各具特色的机器合成音乐，不仅能够作为艺术家的灵感，还极大的充裕了市场，更解决了部分非音乐从业者对原创音乐的渴求。

Google 推出了 MusicLM，这是一种从文本描述中生成高保真音乐的模型，它可以以文本和旋律为条件，根据文本标题中描述的风格转换吹口哨和哼唱的旋律。

方法层面，谷歌采用三个模型来提取音频表示，这些模型将用于条件自回归音乐生成，如图 1 所示。SoundStream 模型用来处理 24 kHz 单声音频，从而得到 50 Hz 的嵌入；具有 600M 参数的 w2v-BERT 模型用于建模中间层；MuLan 模型用于提取目标音频序列的表示。然后将上述得到的离散音频表示与 AudioLM 相结合，从而实现基于文本的音乐生成。为了达到这一效果，谷歌提出了一个分层的序列 - 序列建模任务，其中每个阶段都由单独的解码器 Transformer 自回归建模。

Muzic是 MSRA 领导的一个关于AI音乐的研究项目，它通过深度学习和人工智能赋予音乐理解和生成能力。其任务中就包括作曲、作词等生成任务，其中许多都已经取得了显著的成果。其中，SongMASS 是北京大学 KCL 实验室与南京理工大学、微软亚洲研究院、浙江大学共同提出，发表于 AAAI 2021 的论文《SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint》。该论文提出了 SongMASS，一种歌曲旋律生成和歌词文本生成的方法。它使用扩展的 Masked Seq2Seq （MASS）预训练方法缓解平行语料稀缺的问题，设计句子级和字符级的注意力约束增强歌词和旋律对齐，并在推理阶段使用动态规划算法对齐歌词和旋律。

![sdads](https://s2.loli.net/2023/02/23/RbyDQtwKc8N3BmZ.png)

如模型结构图所示，SongMASS是一个可以在Lyric2Melody和Melody2Lyric两个任务上同时工作的方法。，它采用了基于Transformer (Vaswani et al. 2017)的编码器-解码器框架。由于歌词和旋律之间存在很大的差异，研发人员对歌词和旋律分别采用独立的编码器和解码器，同时为了更好的利用信息，在框架中对歌词到歌词和旋律到旋律进行MASS预训练。我们在歌曲级别对模型进行预训练，以更好地捕捉歌词或旋律序列的长上下文信息，并将监督学习(歌词到旋律和旋律到旋律)纳入预训练，以学习不同模式之间的共享潜在空间。为了学习歌词中的词/音节和旋律中的音符之间的对齐，我们进一步利用句子级约束和标记级约束来指导歌词和旋律之间的对齐。

### 多设备信息共享和调度

目前电子产品已经发展到了一个非常重要的节点，大量IoT设备得到普及，PC和Laptop的性能突飞猛进。伴随技术发展又引出了许多新的技术问题，在多设备协同和信息共享问题上，服务流转成为新的关注焦点。

过往开发者厂家普遍将设备视作独立的生态，设备之间的互动与调度非常少；伴随云服务的发展，许多厂家在智能手机、平板电脑中加入了云存储、备份等功能，实现了以存储为中心的一系列用户信息管理维护，降低了设备之间数据流转的成本，一定程度上带来用户使用体验和工作效率的提升；近年越来越多厂家尝试开发智能手机与PC之间的联动机制，提供在PC上使用或部分使用智能手机功能的方案，使得设备之间的使用场景愈加模糊，用户对不同平台、不同设备、不同生态的感知在逐渐被屏蔽，进一步提高了软件生态的整体性和统一性。

目前，许多大型科技公司，例如 Apple 的i-xxx生态、OPPO的潘塔纳尔操作系统、华为的HarmonyOS以及Google Fuchsia都在跨设备服务流转，是计算能力的融合以及用户零感知方向进行研究。并且，各家的实现方式均有所差异。

以苹果为代表的具有很强生态环境的品牌，已经通过极为稳健的生态系统，形成了独特的多屏互联应用场景。尽管随着硬件生态的丰富，以及Apple OS系统的逐步完善，不同设备之间的系统交互界面越来越趋同，但其实苹果赋予每款硬件设备的专业性丝毫没有降低。对于Apple生态来讲，不同设备首先具有自身的专业功能，更加注重通过其他产品辅助当前使用的产品，强调基础功能的协作以实现注意力的集中。

OPPO的潘塔纳尔系统则更为激进，利用分布式以及软总线特性，逐渐有了去中心化的趋势。通过极简协议的高带宽、低延时以及高安全特性，就能解决在不同设备间的互联“响应”问题，并且可以做到算力在不同设备上的调度和分配。

### 推荐系统

当网络上的数据迅速增加，且用户在某些场景下需求不明确，如何让用户迅速地找到适合自己的消费产品就是推荐系统在生产领域的应用目的。总的来说，推荐系统就是在交互过程中，用户通过行为为系统提供信息，系统为用户提供个性化推荐服务，以此提升用户体验感。

#### 音乐推荐系统

音乐推荐系统与其他领域的系统存在着部分区别，首先音乐的消费时间通常低于5分钟，不同于电影、书籍等持续时间较长的消费品；

## 技术方案

> 初赛阶段完成

### 技术方向

> 指出想要使用技术的所属方向，比如深度学习、IoT等。

本项目将涉及多个领域的技术内容，包括：

- 深度学习与机器学习：用于提取不同设备下信息的特征提取，并在不同的特征空间上对齐
- AIGC：人工智能方法的内容创作
- 推荐系统：将协同过滤与基于内容的过滤相结合的混合推荐算法，在一定程度上解决冷启动和惊喜推荐的问题
- 跨平台客户端：设备端侧使用Flutter+原生方案提供高性能、平台深度结合的跨平台方案

### 技术选择

#### 情绪特征提取

本项目涉及多种模态下特征的提取以及最后的特征匹配，首先要做到对常见模态进行特征提取。

生理特征可以借助OPPO智能穿戴设备提供的API，直接获取。

文本、图像和音频特征的提取我们选择主要依靠 Multimodal Information Bottleneck: Learning Minimal Sufficient Unimodal and Multimodal Representations 模型。

MIB三个单模态学习网络和一个多模态融合网络组成，这些网络的优化由信息瓶颈原理驱动。三种MIB变体的pipeline如下所述：

- **Information Bottleneck**：首先，我们介绍了信息瓶颈(IB)的一般原理。IB的目标是在其复杂性的约束下找到更好的表示，使潜在的表示具有判别性，同时无冗余。IB原则定义了我们所说的良好表示，即简洁表示和良好预测能力之间的基本权衡。具体来说，IB基于互信息(Mutual information, MD)，旨在最大化编码表示与标签之间的MI，以及最小化编码表示与输入之间的MI。MI衡量的是在观察另一个随机变量后，从一个随机变量中获得的信息量。形式上，给定两个随机变量s和y，它们具有联合分布p(a, y和边际分布p(a)和p(g)，它们的MI被定义为联合分布和它们的边际分布乘积之间的kl散度。
- **Procedure of Early-Fusion MIB**：与传统IB不同，在我们的多模态系统中，输入包含三个单模态表示，acm由单模态学习网络Fm (m E {a, o,lY)输出。如何使习得的多模态表示尽可能少地包含来自三个单模态表示的信息仍然是一个问题。一种直观的方法是首先将单模态表示融合为联合的多模态表示，然后通过IB正则化多模态表示，这被称为早期融合MIB (E-MIB)。
  ![das](https://s2.loli.net/2023/02/22/yuHZXcxEWVvqIKL.png)
- **Procedure of Late-Fusion MIB (L-MIB)**：E-MIB的一个缺点是单模态特征可能包含噪声，这对融合过程中跨模态交互的探索产生了负面影响。此外，单峰表征的分布差异可能会严重阻碍模型发现模态之间的互补信息。因此，我们提出了另一个版本的MIB，即后期融合MIB (L-MIB)，它允许单模态表示在融合前过滤掉噪声信息，并通过在L-MIB的每个模态上单独应用IB原理来匹配编码后的单模态分布
- **Procedure of C-MIB**：C-MiB是结合E-MiB和L-MIB的优点而构建的。首先，C-MIB应用IB原则来提取理想情况下无噪声的编码单模表示。然后将编码后的单模态表示进行融合，得到主多模态表示a。进一步将IB原理应用于a，减少冗余，得到最终的多模态表示z。

#### 推荐系统

#### 近场设备通信

任务分发罢了

#### 情绪创作

在视觉创作层面，我们选择使用Stable Diffusion提供的方案。Hugging face在diffusers 中提供了文本引导的Image2Image和Text2Image的生成管线，我们所要做的就是将已有特征重映射到原本使用文本生成的特征空间。

Stable diffusion是一个基于Latent Diffusion Models（潜在扩散模型，LDMs）的文图生成（text-to-image）模型。Latent Diffusion Models是一个可以实现文本生成图像任务的模型，与传统方法和其他深度学习方法比起来，Diffusion model可以取得更好的图片生成效果，然而扩散模型是一种自回归模型，需要反复对自身迭代计算，这一过程会消耗很多时间和计算资源，因此训练和推理代价都很高。为了解决这个问题，Latent Diffusion Model 提出一种在潜在表示空间（latent space）上进行扩散过程的方法，从而能够大大减少计算复杂度，同时也能达到十分不错的图片生成效果。与其他主流空间压缩方法相比，Latent Diffusion Models 提出的方法可以生成更细致的图像，并且具有很好的高分辨率图像生成性能。Latent Diffusion Models模型在无条件图片生成（unconditional image synthesis）, 图片修复（inpainting）,图片超分（super-resolution）等任务上进行了实验，都取得了不错的效果。因此，我们实际上可以通过一定的训练得到不同方式的生成方式，例如无参考图片生成图像、图片风格迁移等。

Latent Diffusion Models还提出了Cross-Attention的方法来实现多模态训练，使得条件图片生成任务也可以实现，这与我们的任务也是契合的。

![xx](https://pic2.zhimg.com/80/v2-691cbd5fe5c322a3b39b3ac2c3af2de5_1440w.webp)

Latent Diffusion Models 主要包含了感知压缩、扩散模型、条件机制三个部分。

**图片感知压缩（Perceptual Image Compression）**：感知压缩可以视作一种训练技巧，并不是并须具有的设计，之前的很多扩散模型没有使用这个技巧也可以进行，但传统非感知压缩的扩散模型有一个很大的问题。由于扩散模型在像素空间上训练模型，如果需要生成一张具有很高分辨率的图片，我们训练的空间也会是一个很高维的空间。引入感知压缩之后，也就是通过VAE这类自编码模型对原图片进行处理，忽略掉图片中的高频信息，只保留重要、基础的一些特征，这一过程类似传统图像处理中的高低频分离，有利于网络学习更有价值的特征。这种方法能够大幅降低训练和采样阶段的计算复杂度，让文图生成等任务能够在消费级GPU（例如搭载8G显存的RTX显卡）上，在10秒级别时间生成图片，大大降低了落地门槛，允许我们将模型同时部署在云端和精简后的设备端。感知压缩的实现过程利用了一个预训练的自编码模型，该模型能够学习到一个在感知上等同于图像空间的潜在表示空间。因此，只需要训练一个通用的自编码模型，就可以用于不同的扩散模型的训练，在不同的任务上使用。

具体来说，给定图像$x \in {\mathbb{R}^{H \times W \times 3}}$，我们可以先利用一个编码器$\varepsilon$来将图像编码到潜在表示空间$z = \varepsilon(x)$上，其中$z\in{\mathbb{R}^{H \times W \times 3}}$，然后再用解码器从潜在表示空间重建图片$\bar{x} = D(z)=D(\varepsilon{x})$。

**潜在扩散模型（Latent Diffusion Models）**：普通的扩散模型（DM）可以解释为一个时序去噪自编码器${\epsilon_{\theta}({x_t},t)};t=1,2,\cdots,T$，其目标是根据输入$x_t$去预测一个对应去噪后的变体，或者说预测噪音，其中$x_t$是输入$x$的噪音版本。相应的目标函数可以写成如下形式：

$$
L_{DM} = {\mathbb{E}_{x,\epsilon \in N(0,1),t}\left[ {\left \| {\epsilon-{\epsilon_\theta}(x_t,t)} \right \|}^2_2  \right]}
$$

而在潜在扩散模型中，引入了预训练的感知压缩模型，它包括一个编码器$\epsilon$和一个解码器$D$。这样就可以利用在训练时就可以利用编码器得到$z_t$，从而让模型在潜在表示空间中学习，相应的目标函数可以写成如下形式：

$$
L_{DM} = {\mathbb{E}_{x,\epsilon \in N(0,1),t}\left[ {\left \| {\epsilon-{\epsilon_\theta}(z_t,t)} \right \|}^2_2  \right]}
$$

**条件机制（Conditioning Mechanisms）**：除了无条件图片生成外，我们也可以进行条件图片生成，这主要是通过拓展得到一个条件时序去噪自编码器（conditional denoising autoencode$\epsilon_\theta(z_t,t,y)$来实现的，这样一来我们就可通过$y$来控制图片合成的过程。具体来说，论文通过在UNet主干网络上增加cross-attention机制来实现$\epsilon_\theta(z_t,t,y)$。为了能够从多个不同的模态预处理$y$，论文引入了一个领域专用编码器（domain specific encoder）$\tau_\theta$，它用来将$\tau$映射为一个中间表示$\tau_\theta(y)\in {\mathbb{R}}^{M\times d}$，这样我们就可以很方便的引入各种形态的条件（文本、类别、layout等等）。最终模型就可以通过一个cross-attention层映射将控制信息融入到UNet的中间层。

音频创作层面使用MSRA团队Muzic项目下的成果。

### 结果期望

> 给出使用该技术的与其结果，注意给出的预期结果应该是合理、可行的。

|       功能       |          技术          |                        预期结果                        |
| :--------------: | :--------------------: | :----------------------------------------------------: |
|   用户信息获取   | 智能穿戴设备、用户输入 |         能够以多种方式不定时获取用户的综合信息         |
|   情绪特征提取   |        深度学习        |         可以提取文本、图片语言等模态的情绪特征         |
|     特征对齐     |        深度学习        |         将提取到的多种特征匹配到同一个特征空间         |
| 交互式反馈和引导 |   深度学习、文本生成   |                           以                           |
|     推荐系统     |        协同过滤        |                                                        |
|     近场通讯     |                        |                                                        |
|    多设备调用    |          HTTP          |            将部分任务分配到合适的设备上计算            |
|     情绪创作     |        深度学习        | 能够根据一定时间一定范围内的用户情绪信息创作一定的作品 |

## 技术实践

> 复赛阶段完成

### 使用的开发框架及依赖的库

### 技术实践过程

## 结果验证

> 复赛阶段完成

## 参考资料

- [多模态情绪识别调研 - CSDN博客](https://blog.csdn.net/qq_39337332/article/details/113513560)
- [何晖光：多模态情绪识别及跨被试迁移学习 - 机器之心](https://www.jiqizhixin.com/articles/2019-05-07-4)
- [基于多任务学习的多模态情绪识别方法](https://www.jsjkx.com/CN/article/openArticlePDF.jsp?id=18633)
- [Multimodal Information Bottleneck: Learning Minimal Sufficient Unimodal and Multimodal Representations](https://arxiv.org/pdf/2210.17444.pdf)
- [EMOTION EMBEDDING SPACES FOR MATCHING MUSIC TO STORIES](https://arxiv.org/pdf/2111.13468.pdf)
- [https://www.jiqizhixin.com/articles/2022-08-16-4?from=synced&keyword=stable%20diffusion](消费级GPU可用，文本转图像开源新模型Stable Diffusion生成宇宙变迁大片)
- [MusicLM来了！谷歌出手解决文本生成音乐问。](https://finance.sina.com.cn/tech/roll/2023-01-28/doc-imyctkkq0277229.shtml)
- [谷歌Magenta项目是如何教神经网络编写音乐的？](https://www.jiqizhixin.com/articles/2016-10-11-2)
- [SongMASS: Automatic Song Writing with Pre-training and Alignment Constraint](https://arxiv.org/pdf/2012.05168.pdf)
- [Stable Diffusion原理解读](https://zhuanlan.zhihu.com/p/583124756)